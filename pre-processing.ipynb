{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZsWMHA6_4TW"
   },
   "source": [
    "<h1>Preprocessing</h1>\n",
    "The sole purpose of this notebook is to pre-process all features making readble to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz-4apHkVSDJ"
   },
   "source": [
    "<h2>Import Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SQdA4m0Y_LJL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n", 
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fzuFkghePQzN"
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "resource_df = pd.read_csv(\"resources.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYP3hIuATGhp"
   },
   "source": [
    "aggregating all values by resource id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rVy_TcNCJ83g"
   },
   "outputs": [],
   "source": [
    "#many columns in the description section has NaN values. for the time being we are filling those fields with NA\n",
    "resource_df[\"description\"] = resource_df[\"description\"].fillna(\"NA\")\n",
    "#adding all fields and grouping on id\n",
    "modified_resource_df = resource_df.groupby('id').agg({'description':'sum', 'quantity':'sum', 'price':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AFTq7iqPfuL"
   },
   "source": [
    "Merging resource data with train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "deFuBLatPXEq"
   },
   "outputs": [],
   "source": [
    "#join dataframes\n",
    "train_df = pd.merge(train_df, modified_resource_df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7gzw53jVV_4"
   },
   "source": [
    "<h2>2. Categorical Features</h2>\n",
    "\n",
    "For categorical features we are going to follow the basic principles of text preprocessing. Such as:\n",
    "\n",
    "1. lowercasing the text\n",
    "2. removal of characters, and symbols\n",
    "3. replacing space with underscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B8SremgTC8-w"
   },
   "outputs": [],
   "source": [
    "#function to clean categorical data\n",
    "def categorical_cleaning(given_data):\n",
    "    data = given_data.copy()\n",
    "    data = data.str.lower()\n",
    "    data = data.str.replace(\".\",\"\")\n",
    "    data = data.str.replace(\" & \",\"_\")\n",
    "    data = data.str.replace(\"-\",\"_\")\n",
    "    data = data.str.replace(' the ','')\n",
    "    data = data.str.replace(\" \",\"_\")\n",
    "    data = data.str.replace(\",\",\"_\")\n",
    "    data = data.str.replace(\"__\",\"_\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMflYMT1C_iB"
   },
   "source": [
    "<h3>2.1 teacher_prefix</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciRi5Mk_VVjg",
    "outputId": "8325b77d-ed12-4593-fceb-736043a236f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: ['Ms.' 'Mrs.' 'Mr.' 'Teacher' 'Dr.' nan]\n",
      "Total rows having missing values: 4\n",
      "Total rows having missing values: 0\n"
     ]
    }
   ],
   "source": [
    "#checking for unique categorical variables and missing values\n",
    "print(\"Unique Values:\",train_df[\"teacher_prefix\"].unique())\n",
    "print(\"Total rows having missing values:\",len(train_df[train_df[\"teacher_prefix\"].isnull()]))\n",
    "\n",
    "#replacing NaN with \"Mrs.\" as the frequncy of Mrs is high\n",
    "train_df[\"teacher_prefix\"] = train_df[\"teacher_prefix\"].fillna(\"Mrs.\")\n",
    "print(\"Total rows having missing values:\",len(train_df[train_df[\"teacher_prefix\"].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qgLTM4UC-42",
    "outputId": "afa1c0b6-f240-4a83-b36b-99b39737e628"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ms', 'mrs', 'mr', 'teacher', 'dr'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning data\n",
    "train_df[\"teacher_prefix_pre\"] = categorical_cleaning(train_df[\"teacher_prefix\"])\n",
    "train_df[\"teacher_prefix_pre\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEmuJ12EZ5G8"
   },
   "source": [
    "<h3>2.2 school_state</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLQ9JThVZ7VK",
    "outputId": "d44c18cc-63e2-4531-e26e-e40c23c5e22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: ['NV' 'GA' 'UT' 'NC' 'CA' 'DE' 'MO' 'SC' 'IN' 'IL' 'VA' 'PA' 'NY' 'FL'\n",
      " 'NJ' 'TX' 'LA' 'ID' 'OH' 'OR' 'MD' 'WA' 'MA' 'KY' 'AZ' 'MI' 'CT' 'AR'\n",
      " 'WV' 'NM' 'WI' 'MN' 'OK' 'AL' 'TN' 'IA' 'KS' 'CO' 'DC' 'WY' 'NH' 'HI'\n",
      " 'SD' 'MT' 'MS' 'RI' 'VT' 'ME' 'NE' 'AK' 'ND']\n",
      "Total rows having missing values: 0\n",
      "The number of codes: 51\n"
     ]
    }
   ],
   "source": [
    "#checking for unique categorical variables and missing values\n",
    "print(\"Unique Values:\",train_df[\"school_state\"].unique())\n",
    "print(\"Total rows having missing values:\",len(train_df[train_df[\"school_state\"].isnull()]))\n",
    "print(\"The number of codes:\",len(train_df[\"school_state\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlWDGBZxaCCc",
    "outputId": "8519a3b5-45ec-44ce-b0be-4e72ad94fc22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nv', 'ga', 'ut', 'nc', 'ca', 'de', 'mo', 'sc', 'in', 'il', 'va',\n",
       "       'pa', 'ny', 'fl', 'nj', 'tx', 'la', 'id', 'oh', 'or', 'md', 'wa',\n",
       "       'ma', 'ky', 'az', 'mi', 'ct', 'ar', 'wv', 'nm', 'wi', 'mn', 'ok',\n",
       "       'al', 'tn', 'ia', 'ks', 'co', 'dc', 'wy', 'nh', 'hi', 'sd', 'mt',\n",
       "       'ms', 'ri', 'vt', 'me', 'ne', 'ak', 'nd'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning data\n",
    "train_df[\"school_state_pre\"] = categorical_cleaning(train_df[\"school_state\"])\n",
    "train_df[\"school_state_pre\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAWCwRA1aazF"
   },
   "source": [
    "<h3>2.3 project_grade_category</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xq3e-uysaYj7",
    "outputId": "08c54734-436b-416f-93c7-471814d0d52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: ['Grades PreK-2' 'Grades 3-5' 'Grades 6-8' 'Grades 9-12']\n",
      "Total rows having missing values: 0\n",
      "The number of grade category: 4\n"
     ]
    }
   ],
   "source": [
    "#checking for unique categorical variables and missing values\n",
    "print(\"Unique Values:\",train_df[\"project_grade_category\"].unique())\n",
    "print(\"Total rows having missing values:\",len(train_df[train_df[\"project_grade_category\"].isnull()]))\n",
    "print(\"The number of grade category:\",len(train_df[\"project_grade_category\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mH63fW3paqQs",
    "outputId": "d4e062dd-b870-42f9-97f8-8900ffab4665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['grades_prek_2', 'grades_3_5', 'grades_6_8', 'grades_9_12'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning data\n",
    "train_df[\"project_grade_category_pre\"] = categorical_cleaning(train_df[\"project_grade_category\"])\n",
    "train_df[\"project_grade_category_pre\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UISRHJYbTP8"
   },
   "source": [
    "<h3>2.4 project_subject_categories</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7lyGjazbQqj",
    "outputId": "46ef4466-212d-454e-e7b3-752905718b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values:\n",
      " ['Literacy & Language' 'Music & The Arts, Health & Sports'\n",
      " 'Math & Science, Literacy & Language' 'Health & Sports'\n",
      " 'Applied Learning, Literacy & Language' 'Math & Science'\n",
      " 'Literacy & Language, Math & Science' 'Special Needs'\n",
      " 'Applied Learning, Special Needs' 'Applied Learning, Music & The Arts']\n",
      "Total rows having missing values: 0\n",
      "The number of unique values: 51\n"
     ]
    }
   ],
   "source": [
    "#checking for unique categorical variables and missing values\n",
    "print(\"Unique Values:\\n\",train_df[\"project_subject_categories\"].unique()[0:10])\n",
    "print(\"Total rows having missing values:\",len(train_df[train_df[\"project_subject_categories\"].isnull()]))\n",
    "print(\"The number of unique values:\",len(train_df[\"project_subject_categories\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lgsNP40bQFb",
    "outputId": "343b462b-7778-4727-f797-2cdb3ff9dd73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['literacy_language', 'music_the_arts_health_sports',\n",
       "       'math_science_literacy_language', 'health_sports',\n",
       "       'applied_learning_literacy_language', 'math_science',\n",
       "       'literacy_language_math_science', 'special_needs',\n",
       "       'applied_learning_special_needs',\n",
       "       'applied_learning_music_the_arts'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning data\n",
    "train_df[\"project_subject_categories_pre\"] = categorical_cleaning(train_df[\"project_subject_categories\"])\n",
    "train_df[\"project_subject_categories_pre\"].unique()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_t8OQlic-RX"
   },
   "source": [
    "<h3>2.5 project_subject_subcategories</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UifsVX5Cc_RA",
    "outputId": "e9c1c39c-8d3a-462e-f880-8e7d0b98cd42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values:\n",
      " ['Literacy' 'Performing Arts, Team Sports'\n",
      " 'Applied Sciences, Literature & Writing' 'Health & Wellness'\n",
      " 'Character Education, Literature & Writing'\n",
      " 'Early Development, Literature & Writing' 'Mathematics'\n",
      " 'Literature & Writing, Mathematics' 'Literacy, Mathematics'\n",
      " 'Character Education, Literacy']\n",
      "Total rows having missing values: 0\n",
      "The number of unique values: 407\n"
     ]
    }
   ],
   "source": [
    "#checking for unique categorical variables and missing values\n",
    "print(\"Unique Values:\\n\",train_df[\"project_subject_subcategories\"].unique()[0:10])\n",
    "print(\"Total rows having missing values:\",len(train_df[train_df[\"project_subject_subcategories\"].isnull()]))\n",
    "print(\"The number of unique values:\",len(train_df[\"project_subject_subcategories\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJzQWrwae6NF",
    "outputId": "1d4dab88-b54f-4c64-cbe2-247bcc88b923"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['literacy', 'performing_arts_team_sports',\n",
       "       'applied_sciences_literature_writing', 'health_wellness',\n",
       "       'character_education_literature_writing',\n",
       "       'early_development_literature_writing', 'mathematics',\n",
       "       'literature_writing_mathematics', 'literacy_mathematics',\n",
       "       'character_education_literacy'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning data\n",
    "train_df[\"project_subject_subcategories_pre\"] = categorical_cleaning(train_df[\"project_subject_subcategories\"])\n",
    "train_df[\"project_subject_subcategories_pre\"].unique()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2Sldm7_fZQ5"
   },
   "source": [
    "<h2>3. Text Features</h2>\n",
    "\n",
    "Below columns are the text features.\n",
    "\n",
    "<i>1. project_title</i>\n",
    "\n",
    "<i>2. project_essay_1</i>\n",
    "\n",
    "<i>3. project_essay_2</i>\n",
    "\n",
    "<i>4. project_essay_3</i>\n",
    "\n",
    "<i>5. project_essay_4</i>\n",
    "\n",
    "<i>6. project_resource_summary</i>\n",
    "\n",
    "<i>7. description</i>\n",
    "\n",
    "Note: \n",
    "\n",
    "1. All essay columns along with project_resource_summary and project_title will be merged into one text corpus as it talks about the project need and details about it.\n",
    "2. We are ignoring description section as thr essay section as everything as description section.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eOBxQsunMDW"
   },
   "source": [
    "<h4>with pre-trained embedding</h4>\n",
    "<i>Here we are using \"glove.840B.300d\" for preprocessing.</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gJB233QRno3Y"
   },
   "outputs": [],
   "source": [
    "#pre saved glove embedding\n",
    "import pickle\n",
    "gloveModel = pickle.load(open(\"./PreprocessingFiles/glove_embedding.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rGeKEpgRE20o"
   },
   "outputs": [],
   "source": [
    "#merging text essay into a single columns\n",
    "train_df[\"project_text\"] = train_df[\"project_title\"].map(str)  + \" \" + train_df[\"project_resource_summary\"].map(str) + \" \" + \\\n",
    "                            train_df[\"project_essay_1\"].map(str) + \" \" + train_df[\"project_essay_2\"].map(str) + \" \" +\\\n",
    "                            train_df[\"project_essay_3\"].map(str) + \" \" +train_df[\"project_essay_4\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "hRbVt3fEGXnJ",
    "outputId": "6c75c805-9e11-4b19-c322-e998485194b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Super Sight Word Centers My students need 6 Ipod Nano\\'s to create and differentiated and engaging way to practice sight words during a literacy station. Most of my kindergarten students come from low-income households and are considered \\\\\"at-risk\\\\\". These kids walk to school alongside their parents and most have never been further than walking distance from their house. For 80% of my students, English is not their first language or the language spoken at home. \\\\r\\\\n\\\\r\\\\nWhile my kindergarten kids have many obstacles in front of them, they come to school each day excited and ready to learn. Most students started the year out never being in a school setting. At the start of the year many had never been exposed to letters. Each day they soak up more knowledge and try their hardest to succeed. They are highly motivated to learn new things every day. We are halfway through the year and they are starting to take off. They know know all letters, some sight words, numbers to 20, and a majority of their letter sounds because of their hard work and determination. I am excited to see the places we will go from here! I currently have a differentiated sight word center that we do daily during our literacy stations. The students have activities that relate to whatever sight word list they are on. This is one of their favorite station activities. I want to continue to provide the students with engaging ways to practice their sight words. \\\\r\\\\n\\\\r\\\\nI dream of having the students use QR readers to scan the sight words that they are struggling with and the Ipods reading the sight words with them. This would help so many of my students by giving them multiple exposures to the words. My students need someone who can go over these sight words daily and I can\\'t always get around to everyone to practice their flashcards with them. With the Ipods they would still have a way to practice their sight words on a daily basis. nan nan'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"project_text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ib-NHjpUcVku"
   },
   "outputs": [],
   "source": [
    "#Source: - https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\n",
    "#some of the known contractions\n",
    "contractions = {\"'aight\": 'alright', \"ain't\": 'am not', \"amn't\": 'am not', \"aren't\": 'are not', \"can't\": 'can not', \"'cause\": \n",
    "'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": \n",
    "'dare not', \"daresn't\": 'dare not', \"dasn't\": 'dare not', \"didn't\": 'did not', \"doesn't\": 'does not', \n",
    "\"don't\": 'do not', 'dunno': \"don't know\", \"d'ye\": 'do you', \"e'er\": 'ever', \"everybody's\": 'everybody is', \n",
    "\"everyone's\": 'everyone is', 'finna': 'fixing to', \"g'day\": 'good day', 'gimme': 'give me', \"giv'n\": 'given', \n",
    "'gonna': 'going to', \"gon't\": 'go not', 'gotta': 'got to', \"hadn't\": 'had not', \"had've\": 'had have', \n",
    "\"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he had', \"he'll\": 'he will', \"he's\": 'he is', \n",
    "\"he've\": 'he have', \"how'd\": 'how did', 'howdy': 'how do you do', \"how'll\": 'how will', \"how're\": 'how are', \n",
    "\"how's\": 'how is', \"I'd\": 'I had', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'm\": 'I am', \n",
    "\"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to', 'innit': 'is it not', \"I've\": 'I have', \"isn't\": 'is not', \n",
    "\"it'd \": 'it would', \"it'll\": 'it will', \"it's \": 'it is', 'iunno': \"I don't know\", \"let's\": 'let us', \n",
    "\"ma'am\": 'madam', \"mayn't\": 'may not', \"may've\": 'may have', 'methinks': 'me thinks', \"mightn't\": 'might not', \n",
    "\"might've\": 'might have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"must've\": 'must have', \n",
    "\"needn't\": 'need not', 'nal': 'and all', \"ne'er\": 'never', \"o'clock\": 'of the clock', \"o'er\": 'over',\n",
    "\"ol'\": 'old', \"oughtn't\": 'ought not', \"'s\": 'is', \"shalln't\": 'shall not', \"shan't\": 'shall not', \n",
    "\"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', \"should've\": 'should have', \n",
    "\"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"somebody's\": 'somebody has', \n",
    "\"someone's\": 'someone has', \"something's\": 'something has', \"so're\": 'so are', \"that'll\": 'that will', \n",
    "\"that're\": 'that are', \"that's\": 'that is', \"that'd\": 'that would', \"there'd\": 'there would', \n",
    "\"there'll\": 'there will', \"there're\": 'there are', \"there's\": 'there is', \"these're\": 'these are', \n",
    "\"they've\": 'they have', \"this's\": 'this is', \"those're\": 'those are', \"those've\": 'those have', \"'tis\": 'it is', \n",
    "\"to've\": 'to have', \"'twas\": 'it was', 'wanna': 'want to', \"wasn't\": 'was not', \"we'd\": 'we would', \n",
    "\"we'd've\": 'we would have', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \n",
    "\"what'd\": 'what did', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what does', \"what've\": 'what have',\n",
    "\"when's\": 'when is', \"where'd\": 'where did', \"where'll\": 'where will', \"where're\": 'where are',\n",
    "\"where's\": 'where is',\"where've\": 'where have', \"which'd\": 'which would', \"which'll\": 'which will', \n",
    "\"which're\": 'which are',\"which's\": 'which is', \"which've\": 'which have', \"who'd\": 'who would',\n",
    "\"who'd've\": 'who would have', \"who'll\": 'who will', \"who're\": 'who are', \"who'ves\": 'who is', \"who'\": 'who have',\n",
    "\"why'd\": 'why did', \"why're\": 'why are', \"why's\": 'why does', \"willn't\": 'will not', \"won't\": 'will not',\n",
    "'wonnot': 'will not', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have',\n",
    "\"y'all\": 'you all', \"y'all'd've\": 'you all would have', \"y'all'd'n've\": 'you all would not have',\n",
    "\"y'all're\": 'you all are', \"cause\":\"because\",\"have't\":\"have not\",\"cann't\":\"can not\",\"ain't\":\"am not\",\n",
    "\"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have', 'cannot': 'can not', \n",
    "'wont': 'will not', \"You'\": 'Am not', \"Ain'\": 'Am not', \"Amn'\": 'Am not', \"Aren'\": 'Are not',\n",
    "\"Can'\": 'Because', \"Could'\": 'Could have', \"Couldn'\": 'Could not have', \"Daren'\": 'Dare not', \n",
    "\"Daresn'\": 'Dare not', \"Dasn'\": 'Dare not', \"Didn'\": 'Did not', \"Doesn'\": 'Does not', \"Don'\": \"Don't know\", \n",
    "\"D'\": 'Do you', \"E'\": 'Ever', \"Everybody'\": 'Everybody is', \"Everyone'\": 'Fixing to', \"G'\": 'Give me', \n",
    "\"Giv'\": 'Going to', \"Gon'\": 'Got to', \"Hadn'\": 'Had not', \"Had'\": 'Had have', \"Hasn'\": 'Has not', \n",
    "\"Haven'\": 'Have not', \"He'\": 'He have', \"How'\": 'How is', \"I'\": 'I have', \"Isn'\": 'Is not', \"It'\": \"I don't know\", \n",
    "\"Let'\": 'Let us', \"Ma'\": 'Madam', \"Mayn'\": 'May not', \"May'\": 'Me thinks', \"Mightn'\": 'Might not', \n",
    "\"Might'\": 'Might have', \"Mustn'\": 'Must not have', \"Must'\": 'Must have', \"Needn'\": 'And all', \"Ne'\": 'Never',\n",
    "\"O'\": 'Old', \"Oughtn'\": 'Is', \"Shalln'\": 'Shall not', \"Shan'\": 'Shall not', \"She'\": 'She is', \n",
    "\"Should'\": 'Should have', \"Shouldn'\": 'Should not have', \"Somebody'\": 'Somebody has', \"Someone'\": 'Someone has', \n",
    "\"Something'\": 'Something has', \"So'\": 'So are', \"That'\": 'That would', \"There'\": 'There is',\n",
    "\"They'\": 'They have', \"This'\": 'This is', \"Those'\": 'It is', \"To'\": 'Want to', \"Wasn'\": 'Was not',\n",
    "\"Weren'\": 'Were not', \"What'\": 'What have', \"When'\": 'When is', \"Where'\": 'Where have', \"Which'\": 'Which have', \n",
    "\"Who'\": 'Who have', \"Why'\": 'Why does', \"Willn'\": 'Will not', \"Won'\": 'Will not', \"Would'\": 'Would have',\n",
    "\"Wouldn'\": 'Would not have', \"Y'\": 'You all are',\"What's\":\"What is\",\"What're\":\"What are\",\"what's\":\"what is\",\n",
    "\"what're\":\"what are\", \"Who're\":\"Who are\", \"your're\":\"you are\",\"you're\":\"you are\", \"You're\":\"You are\",\n",
    "\"We're\":\"We are\", \"These'\": 'These have', \"we're\":\"we are\",\"Why're\":\"Why are\",\"How're\":\"How are \",\n",
    "\"how're \":\"how are \",\"they're \":\"they are \", \"befo're\":\"before\",\"'re \":\" are \",'don\"t ':\"do not\", \n",
    "\"Won't \":\"Will not \",\"could't\":\"could not\", \"would't\":\"would not\", \"We'\": 'We have',\"Hasn't\":\"Has not\",\n",
    "\"n't\":\"not\", 'who\"s':\"who is\"}\n",
    "\n",
    "#characters to keep\n",
    "characters_keep = [']', '◾', '⅞', '☺', '✒', '″', '¬', '•', '=', '❣', '⚠', '©', '°', '}', '⅓', '\"', '✏', '·',\n",
    "                   '-', '?', '…', '|', '\\x96', '.', '›', '¡', '℅', '$', '!', '_', '[', '✔', '{', '´', '’',\n",
    "                   '—', '§', '“', '&', '*', '‘', '―', '¾', '●', '\\\\', '–', '¢', '>', '✂', '¨', '~', ':',\n",
    "                   '”', '`', '™', '\\x80', ',', '@', ';', '<', '(', '#', '❞', '¼', '◦', '❝', '%', '¿', \"'\",\n",
    "                   '+', ')', '½', '®', '/', '→', '⅔', '^']\n",
    "\n",
    "#characters to be removed\n",
    "characters_remove = \"🙏🎧‐﻿😩😋💛️😊🖍🤓‒📏💕😃😀🎶📚🤗💚🤔🏻􏰃‌📖😄💜📓‪💻🏾💗�­􏰁🎭􏰄😉􏰂👍🐣​🖇‑￼★❤■♥\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "8DlmnJpaQuop"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(data, contraction, remove_chars, add_chars):\n",
    "    \n",
    "    #remove characters\n",
    "    translate_table = dict((ord(char), None) for char in remove_chars) \n",
    "    for idx,val in tqdm(enumerate(data.values)):\n",
    "        #remove contractions\n",
    "        val = ' '.join(word.replace(word,contractions[word]) if word in contractions else word for word in val.split())\n",
    "        val = re.sub(r\"n\\'t\", \" not\", val)\n",
    "        val = re.sub(r\"\\'re\", \" are\", val); val = re.sub(r\"\\'s\", \" is\", val); val = re.sub(r\"\\'d\", \" would\", val)\n",
    "        val = re.sub(r\"\\'ll\", \" will\", val); val = re.sub(r\"\\'t\", \" not\", val); val = re.sub(r\"\\'ve\", \" have\", val)\n",
    "        val = re.sub(r\"\\'m\", \" am\", val); val = re.sub(r\"\\\"m \",\" am\", val); val = val.replace(\" nan \",\" \")\n",
    "        val = val.replace(\"nannan\", \" \"); val = val.replace('\\\\r', ' '); val = val.replace('\\\\n', ' ')\n",
    "        val = val.replace('\\\\\"', ' ')\n",
    "        #remove punctuations\n",
    "        val = val.translate(translate_table)\n",
    "        #spacing of characters\n",
    "        for char in add_chars:  \n",
    "            if char in val:\n",
    "                val = val.replace(char,\" \"+char+\" \")\n",
    "                val = re.sub(\"\\s+\",\" \",val)\n",
    "        #adding preprocessed sentence\n",
    "        val = re.sub(\"\\s+\",\" \",val)\n",
    "        data.values[idx] = val.strip()\n",
    "    \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BY2TJ6UM2gGy",
    "outputId": "d2cbd534-bdd0-443a-bc37-75576898fc2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182080it [02:01, 1504.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"project_text\"] = text_preprocessing(train_df[\"project_text\"], contractions, characters_remove, characters_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "agNbnjcMY4r9"
   },
   "outputs": [],
   "source": [
    "#creating text vocab\n",
    "def question_text_vocab(text):\n",
    "    freq_dict = defaultdict(int)\n",
    "    total_sent = text.apply(lambda x: x.split()).values\n",
    "    for sent in total_sent:\n",
    "        for token in sent:\n",
    "            freq_dict[token] += 1\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "y_-Wz5QJY4hW"
   },
   "outputs": [],
   "source": [
    "#checking coverage for words present in question_text and in embedding_matrix\n",
    "def coverage(vocab, embedding,print_statement=False):\n",
    "    #Initializing values\n",
    "    known_words = defaultdict(int)\n",
    "    unknown_words = defaultdict(int)\n",
    "    knownWordsVal = 0\n",
    "    unknownWordsVal = 0\n",
    "    #iterating words\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embedding[word]\n",
    "            knownWordsVal += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            unknownWordsVal += vocab[word]\n",
    "            pass\n",
    "    \n",
    "    if print_statement == True:\n",
    "        print('Found {:.2%} of words in the embedding of the question text vocab'\n",
    "           .format(len(known_words) / len(vocab)))\n",
    "        print('Found {:.2%} of the words in the question text vocab'.\n",
    "              format(knownWordsVal / (knownWordsVal + unknownWordsVal)))\n",
    "    else:\n",
    "        pass\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CE2Bvc5dqjo",
    "outputId": "7c5607dc-b529-4319-f1c5-ca50264ab9e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85.23% of words in the embedding of the question text vocab\n",
      "Found 99.92% of the words in the question text vocab\n"
     ]
    }
   ],
   "source": [
    "text_vocab_tr = question_text_vocab(train_df[\"project_text\"])\n",
    "oov_glove = coverage(text_vocab_tr, gloveModel, print_statement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQNGU1jk_E5Z"
   },
   "source": [
    "As we can see here there 99.92% of the words of the train vocab is now covered by out pre trained embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSOk3FVaPotd"
   },
   "source": [
    "without pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2uSZVAPiPzwU"
   },
   "outputs": [],
   "source": [
    "#merging text essay into a single columns\n",
    "train_df[\"project_text_wt_emb\"] = train_df[\"project_title\"].map(str)  + \" \" + train_df[\"project_resource_summary\"].map(str) + \" \" + \\\n",
    "                                train_df[\"project_essay_1\"].map(str) + \" \" + train_df[\"project_essay_2\"].map(str) + \" \" +\\\n",
    "                                train_df[\"project_essay_3\"].map(str) + \" \" +train_df[\"project_essay_4\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MOLQV1L3QrGm"
   },
   "outputs": [],
   "source": [
    "#some defined stopwords\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "\"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "'ve', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "\"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "\"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "x83ew0lPPztr"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing_without_embedding(data, contractions, stopwords):\n",
    "    \n",
    "    for idx,val in tqdm(enumerate(data.values)):\n",
    "        #remove contractions\n",
    "        val = ' '.join(word.replace(word,contractions[word]) if word in contractions else word for word in val.split())\n",
    "        val = re.sub(r\"n\\'t\", \" not\", val)\n",
    "        val = re.sub(r\"\\'re\", \" are\", val); val = re.sub(r\"\\'s\", \" is\", val); val = re.sub(r\"\\'d\", \" would\", val)\n",
    "        val = re.sub(r\"\\'ll\", \" will\", val); val = re.sub(r\"\\'t\", \" not\", val); val = re.sub(r\"\\'ve\", \" have\", val)\n",
    "        val = re.sub(r\"\\'m\", \" am\", val); val = re.sub(r\"\\\"m \",\" am\", val); val = val.replace(\" nan \",\" \")\n",
    "        val = val.replace(\"nannan\", \" \"); val = val.replace('\\\\r', ' '); val = val.replace('\\\\n', ' ')\n",
    "        val = val.replace('\\\\\"', ' ')\n",
    "        #removing characters and punctuation\n",
    "        val = re.sub('[^A-Za-z0-9]+', ' ', val)\n",
    "        #removing stopwords\n",
    "        val = ' '.join(e for e in val.split() if e.lower() not in stopwords)\n",
    "        #adding preprocessed sentence\n",
    "        val = re.sub(\"\\s+\",\" \",val)\n",
    "        data.values[idx] = val.strip()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QV-amLROPzqb",
    "outputId": "99afa845-6f41-446a-bdee-200f2bf84051"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182080it [02:17, 1325.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"project_text_wt_emb\"] = text_preprocessing_without_embedding(train_df[\"project_text_wt_emb\"], contractions, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FqujPRjSHry"
   },
   "source": [
    "Saving pre-processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ncUzXElEBDpR"
   },
   "outputs": [],
   "source": [
    "#saving data\n",
    "train_df.to_csv(\"preprocessed_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessing_with_Embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
